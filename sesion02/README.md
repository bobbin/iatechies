# Sesi√≥n 02: Tokens y Tokenizaci√≥n

Colecci√≥n de ejercicios pr√°cticos para entender, contar y visualizar tokens en modelos de lenguaje.

## Objetivos de Aprendizaje

Al finalizar esta sesi√≥n, ser√°s capaz de:

- ‚úÖ Entender qu√© es un token y c√≥mo funciona la tokenizaci√≥n
- ‚úÖ Contar tokens de forma precisa y estimar costos
- ‚úÖ Visualizar tokens para comprender el proceso
- ‚úÖ Identificar trampas comunes que disparan el conteo de tokens
- ‚úÖ Comparar diferentes textos y optimizar prompts

## Requisitos

- Python 3.10+
- Ollama corriendo en local (http://localhost:11434) - **requerido para ejercicios 07-10**
- Instala dependencias:
  ```bash
  pip install -r requirements.txt
  ```
  
  O simplemente:
  ```bash
  pip install tiktoken requests
  ```

## Ejercicios

### 01_que_es_token.py
**Introducci√≥n al concepto de token**

Muestra c√≥mo se tokeniza un texto usando `tiktoken` y qu√© son los tokens.

- Ejemplos simples de tokenizaci√≥n
- Visualizaci√≥n de tokens individuales
- Comparaci√≥n de ratios

**Conceptos clave:**
- Los tokens son IDs num√©ricos que representan partes del texto
- No siempre coinciden con palabras completas
- Cada token puede ser decodificado de vuelta a texto

---

### 02_contar_tokens.py
**M√©todos para contar tokens**

Compara m√©todos de conteo real vs estimaciones.

- Conteo real con `tiktoken`
- Estimaciones simples (caracteres/4, palabras*1.3)
- Comparaci√≥n de errores

**Conceptos clave:**
- El conteo exacto requiere tokenizaci√≥n real
- Las estimaciones pueden tener errores significativos
- Para facturaci√≥n precisa, siempre usa `tiktoken`

---

### 03_visualizar_tokens.py
**Visualizaci√≥n de tokens**

Muestra tokens con colores para entender c√≥mo se divide el texto.

- Visualizaci√≥n con colores ANSI
- Desglose detallado de cada token
- Comparaci√≥n visual entre textos

**Conceptos clave:**
- Cada token se muestra con un color diferente
- Los tokens pueden decodificarse de vuelta a texto
- La visualizaci√≥n ayuda a entender la tokenizaci√≥n

---

### 04_trampas_tokenizacion.py
**Casos extremos y sorpresas**

Muestra casos donde el conteo de tokens puede ser inesperado.

- Emojis (consumen m√°s tokens)
- C√≥digo (ratios diferentes)
- Idiomas (chino, japon√©s, etc.)
- Comparaci√≥n de ratios

**Conceptos clave:**
- Emojis pueden consumir 2-3 tokens cada uno
- El c√≥digo se tokeniza de forma diferente
- Idiomas no latinos requieren m√°s tokens
- Siempre mide antes de desplegar

---

### 05_comparar_textos.py
**Comparaci√≥n y an√°lisis**

Compara c√≥mo diferentes textos se tokenizan.

- Comparaci√≥n por longitud
- Comparaci√≥n por idioma
- Ejemplo de optimizaci√≥n (encontrar mejor variante)

**Conceptos clave:**
- Diferentes textos tienen ratios diferentes
- La longitud no es proporcional a tokens
- Comparar variantes ayuda a optimizar prompts

---

### 06_tokens_interactivo.py
**Tokenizador Interactivo**

Herramienta interactiva para experimentar con tokenizaci√≥n.

- Escribe texto y ve el an√°lisis completo
- Visualizaci√≥n con colores en tiempo real
- Estad√≠sticas detalladas (ratios, costos)
- Desglose de cada token individual

**Conceptos clave:**
- Experimenta libremente con diferentes textos
- Visualizaci√≥n inmediata de c√≥mo se tokeniza
- √ötil para optimizar prompts antes de producci√≥n

---

### 07_limites_contexto.py
**L√≠mites de Contexto**

Demuestra los l√≠mites de contexto y qu√© pasa cuando se exceden.

- Pruebas con diferentes tama√±os
- Identificaci√≥n de l√≠mites m√°ximos
- Qu√© pasa al exceder el l√≠mite
- Medici√≥n de impacto

**Conceptos clave:**
- Cada modelo tiene un l√≠mite m√°ximo
- Exceder causa errores o truncado
- M√°s grande no siempre es mejor

---

### 08_latencia_contexto.py
**Latencia y Contexto**

Muestra c√≥mo el tama√±o del contexto afecta la latencia.

- Medici√≥n de latencia seg√∫n tama√±o
- An√°lisis de crecimiento no lineal
- Impacto en eficiencia
- Cuellos de botella

**Conceptos clave:**
- M√°s contexto = m√°s latencia
- Crecimiento puede ser exponencial
- Hardware exponencialmente m√°s potente
- Optimizar contexto es clave

---

### 09_tecnicas_contexto.py
**T√©cnicas para Manejar Contexto**

Demuestra t√©cnicas para procesar textos largos.

- Truncado simple
- Ventanas deslizantes
- Chunking por oraciones
- Comparaci√≥n de t√©cnicas

**Conceptos clave:**
- Truncado: simple pero pierde informaci√≥n
- Ventanas: procesa todo con solapamiento
- Chunking: mantiene coherencia sem√°ntica
- Cada t√©cnica tiene su caso de uso

---

### 10_afecta_prompt.py
**C√≥mo Afecta el Prompt al Contexto**

Muestra c√≥mo diferentes estructuras afectan el contexto.

- Comparaci√≥n de estructuras de prompt
- Impacto en tokens y tiempo
- Ejemplos de optimizaci√≥n
- Recomendaciones

**Conceptos clave:**
- Estructura del prompt determina tokens
- M√°s palabras = m√°s tokens
- Optimizaci√≥n reduce costos
- Medir siempre antes de optimizar

---

### 11_sliding_windows.py
**Sliding Windows (Ventanas Deslizantes)**

Procesa textos largos en ventanas solapadas y agrega resultados.

- Creaci√≥n de ventanas por tokens
- Medici√≥n por ventana (tokens/tiempo) y total
- Resumen agregado

**Conceptos clave:**
- Solapamiento preserva contexto local
- Ajuste de tama√±o/solapamiento afecta coste/calidad
- √ötil para documentos que exceden el contexto

## Uso

### Ejecutar un ejercicio individual

```bash
# Ejercicios de tokens (solo requieren tiktoken)
python 01_que_es_token.py
python 02_contar_tokens.py
python 03_visualizar_tokens.py
python 04_trampas_tokenizacion.py
python 05_comparar_textos.py
python 06_tokens_interactivo.py

# Ejercicios de contexto (requieren Ollama corriendo)
python 07_limites_contexto.py
python 08_latencia_contexto.py
python 09_tecnicas_contexto.py
python 10_afecta_prompt.py
python 11_sliding_windows.py

# Ejercicios de embeddings
cd embeddings
python 01_inspector_basico.py
python 02_busqueda_titulares.py
python 03_multilingue.py
python 04_deduplicador.py
python 05_tags_inteligentes.py
```

### Flujo de aprendizaje recomendado

**Parte 1: Tokens**
1. **Ejercicio 01**: Entender el concepto b√°sico
2. **Ejercicio 02**: Aprender a contar tokens
3. **Ejercicio 03**: Ver c√≥mo se tokeniza
4. **Ejercicio 04**: Conocer casos extremos
5. **Ejercicio 05**: Analizar y optimizar
6. **Ejercicio 06**: Herramienta interactiva

**Parte 2: Contexto**
7. **Ejercicio 07**: Entender l√≠mites de contexto
8. **Ejercicio 08**: Ver impacto en latencia
9. **Ejercicio 09**: Aprender t√©cnicas de manejo
10. **Ejercicio 10**: Optimizar prompts

## Conceptos Importantes

### ¬øQu√© es un Token?

Un token es la unidad m√≠nima de procesamiento en un texto. Seg√∫n el algoritmo:
- Puede ser un car√°cter, s√≠laba o subcadena
- No siempre coincide con palabras completas
- Cada token tiene un ID num√©rico √∫nico

### Algoritmos de Tokenizaci√≥n

- **BPE (Byte Pair Encoding)**: Combina pares frecuentes de bytes
- **SentencePiece**: Opera a nivel de caracteres Unicode
- **Unigram**: Probabil√≠stico basado en frecuencias

### Impacto en Costos

El recuento de tokens impacta directamente en:
- ‚è±Ô∏è Tiempo de procesamiento
- üí∞ Costo de las APIs
- üß† Consumo de memoria
- üìä Rendimiento del modelo

### L√≠mites de Contexto

- **Ventana de contexto**: M√°ximo de tokens que un modelo puede procesar simult√°neamente
- **Exceder l√≠mites**: Puede causar errores, truncado o degradaci√≥n de calidad
- **Latencia**: M√°s contexto = m√°s tiempo de procesamiento (a veces exponencial)
- **T√©cnicas**: Truncado, ventanas deslizantes, chunking para manejar textos largos
- **Optimizaci√≥n**: Estructura del prompt afecta significativamente el uso del contexto

## Trampas Comunes

### ‚ö†Ô∏è Casos Extremos
- Emojis: 2-3 tokens cada uno
- S√≠mbolos especiales: M√∫ltiples tokens
- C√≥digo: Tokenizaci√≥n sub√≥ptima

### üåç Impacto Multiling√ºe
- Chino, Japon√©s: M√°s tokens por car√°cter
- √Årabe, Ruso: Ratios diferentes
- Espa√±ol/Ingl√©s: Generalmente m√°s eficientes

### üîß Herramientas de Debug
- Visualizadores de tokens
- Testing antes de producci√≥n
- Monitoreo en tiempo real

## Tips de Optimizaci√≥n

1. **Minimiza emojis** en prompts de producci√≥n
2. **Evita s√≠mbolos innecesarios**
3. **Optimiza c√≥digo** antes de incluirlo
4. **Considera el idioma** al planificar costos
5. **Mide siempre** antes de escalar

## Archivos Generados

Algunos ejercicios generan archivos:
- `tokens_visualizacion.txt`: Visualizaci√≥n exportada
- `comparacion_tokens.csv`: Comparaciones en formato CSV

## Notas Importantes

- Usamos `tiktoken` con el encoding `cl100k_base` (el mismo que GPT-4)
- Los resultados son precisos y consistentes
- Puedes cambiar el encoding seg√∫n necesites (p50k_base, r50k_base, etc.)
- La visualizaci√≥n muestra los tokens reales del modelo

## Pr√≥ximos Pasos

Despu√©s de completar estos ejercicios, podr√°s:
- Optimizar tus prompts para reducir tokens
- Predecir costos con mayor precisi√≥n
- Identificar problemas de tokenizaci√≥n antes de producci√≥n
- Tomar decisiones informadas sobre formato de datos

## Recursos Adicionales

- [Documentaci√≥n de Ollama](https://ollama.com)
- Herramientas de visualizaci√≥n de tokens online
- Tokenizer playgrounds de Hugging Face
- Extensi√≥n de navegador para visualizar tokens

---

¬°Disfruta explorando el mundo de los tokens! üöÄ
