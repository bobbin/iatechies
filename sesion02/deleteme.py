import pymysql import json import boto3 import os import uuid import urllib.parse from PIL import Image import io import fitz # PyMuPDF import base64 import vertexai from vertexai.generative_models import GenerativeModel, Part, FinishReason import vertexai.preview.generative_models as generative_models from google.oauth2 import service_account import random import time from datetime import datetime s3 = boto3.client('s3') def get_credentials(): credentials_json = os.environ['GOOGLE_APPLICATION_CREDENTIALS'] try: credentials_info = json.loads(credentials_json) except json.JSONDecodeError: raise ValueError("El contenido de GOOGLE_APPLICATION_CREDENTIALS no es un JSON válido") credentials = service_account.Credentials.from_service_account_info(credentials_info) return credentials def generate(file_content, mime_type): """ Envía el contenido de un fichero (PDF o imagen) directamente a Gemini. """ print("Generando análisis del documento...") # Crea el objeto 'Part' a partir del contenido del fichero y su MIME type document_part = Part.from_data(mime_type=mime_type, data=file_content) credentials = get_credentials() vertexai.init(project="fastsumm-45609", location="europe-west1", credentials=credentials) model = GenerativeModel("gemini-2.5-flash-lite") #Recomiendo usar gemini-1.5-flash-001 para la lectura de PDFs # El prompt no necesita cambios, sigue siendo válido text1 = """ Eres un sistema experto en extracción de datos estructurados de Documentos de Identificación de Traslado de Residuos (RD 553/2020). Recibirás el contenido visual de un documento en formato imagen o pdf, que puede tener varias páginas. Debes extraer los siguientes campos y devolverlos como JSON válido. Si un dato no está presente, devuelve null. No inventes datos. Busca en las secciones específicas: - Documento de Identificación: numero_di (Nº DI), numero_nota (Nº NT), fecha_traslado. - Operador de traslado: nif_operador_translado, nima_operador_translado, nombre_operador_translado, direccion_operador_translado, cp_operador_translado, municipio_operador_translado, provincia_operador_translado, ccaa_operador_translado. - Origen del traslado: tipo_origen, centro_origen, razon_social_origen, cif_origen, nima_origen, direccion_origen, cp_origen, municipio_origen, provincia_origen, ccaa_origen. - Destino del traslado: tipo_destino, razon_social_destino, cif_destino, nima_destino, direccion_destino, cp_destino, municipio_destino, provincia_destino, ccaa_destino, tratamiento. - Transportista: tipo_transportista, razon_social_transportista, cif_transportista, nima_transportista, direccion_transportista, cp_transportista, municipio_transportista, provincia_transportista, ccaa_transportista. - Residuo: Puede venir como información sobre el residuo que se traslada, y debes extraer los campos: residuo, código ler, cantidad recibida o aceptada. Respecto a los campos "tipo", tienen nombres distintos en el documento y puede que no estén en todos los documentos, esta es la relación: - tipo_origen: Lo encontrarás como Tipo Centro, en la parte de Origen del Traslado, por ejemplo: P02 - tipo_destino: Lo encontrarás como Tipo Centro Gestor, en la parte de Destino del Traslado, por ejemplo: G01 - tipo_transportista: Lo encontrarás como Tipo de Operador, en la parte de Operador de Traslado, por ejemplo: P02 Devuelve el siguiente JSON: { "numero_di": null, "numero_nota": null, "fecha_traslado": null, "nif_operador_translado": null, "nima_operador_translado": null, "nombre_operador_translado": null, "direccion_operador_translado": null, "cp_operador_translado": null, "municipio_operador_translado": null, "provincia_operador_translado": null, "ccaa_operador_translado": null, "tipo_origen": null, "centro_origen": null, "razon_social_origen": null, "cif_origen": null, "nima_origen": null, "direccion_origen": null, "cp_origen": null, "municipio_origen": null, "provincia_origen": null, "ccaa_origen": null, "tipo_destino": null, "razon_social_destino": null, "cif_destino": null, "nima_destino": null, "direccion_destino": null, "cp_destino": null, "municipio_destino": null, "provincia_destino": null, "ccaa_destino": null, "tipo_transportista": null, "razon_social_transportista": null, "cif_transportista": null, "nima_transportista": null, "direccion_transportista": null, "cp_transportista": null, "municipio_transportista": null, "provincia_transportista": null, "ccaa_transportista": null, "residuo": null, "ler": null, "tratamiento": null, "cantidad": null } """ generation_config = { "max_output_tokens": 25000, "temperature": 0.1, "top_p": 0.95, "response_mime_type": "application/json" } safety_settings = { generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH, generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH, generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH, generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH, } # Se envía el documento (PDF o imagen) y el prompt de texto a Gemini response = model.generate_content( [document_part, text1], generation_config=generation_config, safety_settings=safety_settings, stream=False, ) print(response.text) return json.loads(response.text) def save_to_mysql(data, documento): """ Guarda los datos en la tabla principal 'registros_ramon' de forma idempotente. Primero comprueba si el registro ya existe para evitar duplicados. """ conn = pymysql.connect( host='inproget-dev.cp09dvi98cfg.eu-west-1.rds.amazonaws.com', user='admin', password='4AO6B26wblxivPjctjUA', database='inproget' ) cursor = conn.cursor() try: # --- PASO 1: VERIFICAR SI EL DOCUMENTO YA EXISTE --- cursor.execute("SELECT id FROM registros_ramon WHERE documento_origen = %s", (documento,)) result = cursor.fetchone() if result: print(f"INFO: El documento '{documento}' ya existe en 'registros_ramon'. No se insertará de nuevo.") return # Salir de la función si el registro ya existe # --- PASO 2: SI NO EXISTE, PROCEDER CON LA INSERCIÓN --- print(f"INFO: Insertando nuevo registro para el documento '{documento}' en 'registros_ramon'.") columns = [ "numero_di","numero_nota","fecha_traslado", "nif_operador_translado","nima_operador_translado","nombre_operador_translado","direccion_operador_translado","cp_operador_translado","municipio_operador_translado","provincia_operador_translado","ccaa_operador_translado", "tipo_origen","centro_origen","razon_social_origen","cif_origen","nima_origen","direccion_origen","cp_origen","municipio_origen","provincia_origen","ccaa_origen", "tipo_destino","razon_social_destino","cif_destino","nima_destino","direccion_destino","cp_destino","municipio_destino","provincia_destino","ccaa_destino", "tipo_transportista","razon_social_transportista","cif_transportista","nima_transportista","direccion_transportista","cp_transportista","municipio_transportista","provincia_transportista","ccaa_transportista", "residuo","ler","tratamiento","cantidad" ] columns.append("documento_origen") placeholders = ",".join(["%s"] * len(columns)) sql = f"INSERT INTO registros_ramon ({','.join(columns)}) VALUES ({placeholders})" values = [ normalize_date(data.get(col)) if col == "fecha_traslado" else data.get(col) for col in columns[:-1] ] + [documento] cursor.execute(sql, values) conn.commit() print(f"ÉXITO: Registro para '{documento}' insertado correctamente en 'registros_ramon'.") except pymysql.Error as e: conn.rollback() print(f"ERROR: Error de MySQL en 'save_to_mysql' para el documento '{documento}': {str(e)}") # Lanzamos la excepción para que SQS sepa que algo falló y pueda reintentar raise e finally: cursor.close() conn.close() def normalize_date(date_str): # Esta función no necesita cambios if not date_str or not isinstance(date_str, str): return None for fmt in ("%d/%m/%Y", "%d-%m-%Y", "%Y-%m-%d"): try: return datetime.strptime(date_str.strip(), fmt).strftime("%Y-%m-%d") except ValueError: continue return None def save_to_mysql_log(data, document_key): # Esta función no necesita cambios conn = pymysql.connect( host='inproget-dev.cp09dvi98cfg.eu-west-1.rds.amazonaws.com', user='admin', password='4AO6B26wblxivPjctjUA', database='inproget' ) cursor = conn.cursor() columns = [ "numero_di","numero_nota","fecha_traslado", "nif_operador_translado","nima_operador_translado","nombre_operador_translado","direccion_operador_translado","cp_operador_translado","municipio_operador_translado","provincia_operador_translado","ccaa_operador_translado", "tipo_origen","centro_origen","razon_social_origen","cif_origen","nima_origen","direccion_origen","cp_origen","municipio_origen","provincia_origen","ccaa_origen", "tipo_destino","razon_social_destino","cif_destino","nima_destino","direccion_destino","cp_destino","municipio_destino","provincia_destino","ccaa_destino", "tipo_transportista","razon_social_transportista","cif_transportista","nima_transportista","direccion_transportista","cp_transportista","municipio_transportista","provincia_transportista","ccaa_transportista", "residuo","ler","tratamiento","cantidad" ] columns.append("documento_origen") placeholders = ",".join(["%s"] * len(columns)) sql = f"INSERT INTO processed_log_ramon ({','.join(columns)}) VALUES ({placeholders})" values = [ normalize_date(data.get(col)) if col == "fecha_traslado" else data.get(col) for col in columns[:-1] ] + [document_key] try: cursor.execute(sql, values) conn.commit() print(f"Registro de log insertado correctamente.") except pymysql.Error as e: conn.rollback() print(f"Error al insertar el registro de log en MySQL: {str(e)}") finally: cursor.close() conn.close() def lambda_handler(event, context): print("Evento SQS recibido:", json.dumps(event)) for sqs_message in event['Records']: try: s3_event_notification = json.loads(sqs_message['body']) s3_record = s3_event_notification['Records'][0] bucket = s3_record['s3']['bucket']['name'] key = urllib.parse.unquote_plus(s3_record['s3']['object']['key']) print(f"Procesando fichero: s3://{bucket}/{key}") # --- Determinar el MIME type a partir de la extensión del fichero --- file_extension = os.path.splitext(key)[1].lower() if file_extension == ".pdf": mime_type = "application/pdf" elif file_extension in [".jpg", ".jpeg"]: mime_type = "image/jpeg" elif file_extension == ".png": mime_type = "image/png" else: # Si el tipo de fichero no es soportado, se salta el mensaje print(f"Fichero con extensión no soportada: {key}. Saltando procesamiento.") continue # --- Leer el contenido del fichero desde S3 --- response = s3.get_object(Bucket=bucket, Key=key) file_content = response['Body'].read() # --- Enviar el fichero directamente a Gemini --- extracted_data = generate(file_content, mime_type) # --- Guardar los datos en la base de datos --- save_to_mysql(extracted_data, key) save_to_mysql_log(extracted_data, key) print(f"Fichero {key} procesado y datos guardados correctamente.") except Exception as e: print(f"ERROR procesando mensaje SQS con ID {sqs_message.get('messageId')}.") print(f"Error: {e}") continue return { 'statusCode': 200, 'body': json.dumps('Procesamiento de lote SQS finalizado.') }